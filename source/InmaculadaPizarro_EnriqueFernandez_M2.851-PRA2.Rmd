---
title: 'Tipología y Ciclo de vida del Dato: Práctica 2: Limpieza y validación de los datos'
author: "Autores: Inmaculada Pizarro Moreno y Enrique Fernández Morales"
date: "Enero 2023"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: M2.851-PRA2-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
geometry: margin=20mm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Descripción del dataset

En el siguiente ejercicio nos disponemos a analizar el conjunto de datos sugerido para la PRA2 de tipología y ciencia de vida del dato sobre ataques de corazón y variables asociadas a cada muestra. 

El conjunto de datos objeto de análisis se ha obtenido a partir de este enlace en Kaggle:
[Kaggle dataset sobre ataques de corazón](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset)

y está constituido por 14 características (columnas) que presentan 303 pacientes (filas o registros).
Entre los campos de este conjunto de datos, encontramos los siguientes:

+ **age**             Edad del paciente
+ **sex**             Sexo del paciente
  - Value 0: mujer
  - Value 1: hombre
+ **cp**              Tipo de dolor en el pecho
  - Value 0: angina típica
  - Value 1: angina atípica
  - Value 2: dolor no de angina
  - Value 3: asintomático
+ **trtbps**        Presión arterial en reposo (mm Hg)
+ **chol**          Colestoral en mg/dl obtenido a través del sensor BMI
+ **fbs**           Azúcar en sangre en ayunas > 120 mg/dl
  - Value 0: falso
  - Value 1: verdadero
+ **restecg**       Resultados electrocardiográficos en reposo
  - Value 0: normal
  - Value 1: tener anomalías en la onda ST-T (inversiones de la onda T y/o elevación o depresión del ST > 0,05 mV)
  - Value 2: que muestra hipertrofia ventricular izquierda probable o definitiva según los criterios de Estes
+ **thalachh**        Frecuencia cardíaca máxima alcanzada
+ **exng**            Angina inducida por el ejercicio
  - Value 0: no
  - Value 1: sí
+ **oldpeak**         Pico Anterior
+ **slp**             La pendiente del segmento ST de ejercicio máximo
  - Value 0: pendiente descendente
  - Value 1: plano
  - Value 1: pendiente ascendente
+ **caa**             Número de vasos principales (0-3)
+ **thall**           Thalassemia: trastorno genético de la sangre que se caracteriza por una tasa de hemoglobina más baja de lo normal. Resultado de la prueba de esfuerzo con talio ~ (0,3)
  - Value 0: nada
  - Value 1: defecto fijo
  - Value 2: normal
  - Value 3: defecto reversible
+ **output**          Posibilidad de infarto
  - Value 0: menos posibilidades de infarto
  - Value 1: más posibilidades de infarto


# Importancia y objetivos de los análisis

Mediante el análisis de este dataset sobre pacientes de corazón nos planteamos saber si alguna de las pruebas realizadas en ellos podría ser determinante en que la posibilidad de infarto sea mayor o menor para el paciente.

Este tipo de análisis es muy común en el sector de la salud y presenta grandes retos, teniendo beneficios enormes en la investigación de todo tipo de enfermedades y tratamientos. Como ejemplo tenemos este mismo caso que trataremos en esta practica, donde el conocimiento que podemos extraer a la hora de tratar este tipo de datos puede ayudarnos a predecir infartos y salvar vidas.

Nos resulta también interesante saber qué variables se correlacionan entre ellas, de modo que podamos de algún modo no sólo saber la importancia de cada variable en el resultado sino la dependencia entre ellas, ya que podría ser útil a la hora de atacar el problema por algún camino médico, por ejemplo, el colesterol con respecto a los problemas de presión arterial o viceversa. Además, también nos interesa realizar otro tipo de pruebas estadísticas, como por ejemplo el contraste de hipótesis para comprobar las diferencias estadísticas significativas entre grupos de datos, y como por ejemplo realizar un modelo de regresión lineal para analizar la relación entre algunas variables.

Estos análisis nos permiten inferir propiedades sobre el resto de la población de pacientes de corazón.

# Limpieza de los datos

El primer paso es cargar los datos para trabajar con ellos haciendo uso de la función read.csv de la librería readr.

```{r message= FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
if(!require('Rmisc')) install.packages('Rmisc'); library('Rmisc')
if(!require('dplyr')) install.packages('dplyr'); library('dplyr')
if(!require('xfun')) install.packages('xfun'); library('xfun')
if (!require('tidyverse')) install.packages('tidyverse'); library(tidyverse)
if (!require('readr')) install.packages('readr'); library(readr)
if (!require('hrbrthemes')) install.packages('hrbrthemes'); library(hrbrthemes)
if (!require('viridis')) install.packages('viridis'); library(viridis)
if (!require('forcats')) install.packages('forcats'); library(forcats)
if (!require('extrafont')) install.packages('extrafont'); library(extrafont)
if(!require('formattable')) install.packages('formattable'); library('formattable')
if(!require('gridExtra')) install.packages('gridExtra'); library('gridExtra')
if(!require('grid')) install.packages('grid'); library('grid')
if(!require('ggpubr')) install.packages('ggpubr'); library('ggpubr')
if (!require('ggthemes')) install.packages("ggthemes");library(ggthemes)
if (!require('nortest')) install.packages('nortest'); library('nortest')
```

```{r message= FALSE, warning=FALSE}
pacientes <- read.csv('../dataset/heart.csv', header = TRUE, sep = ',')
```

Vemos qué clase tiene cada variable usando la función sapply que nos permite aplicar la función class sobre todo data frame.

```{r  message= FALSE, warning=FALSE}
# Tipo de dato asignado a cada campo
sapply(pacientes, function(x) class(x))
```

Comprobemos de nuevo las características y estadísticas de los datos originales.

```{r  message= FALSE, warning=FALSE}
glimpse(pacientes)
```

Nos ha clasificado automáticamente todas las variables como integer o numeric.  Sabemos por su descripción y valores posibles que varias de las variables son discretas y deberían ser factorizadas.  Para ello usamos la función lapply y le pasamos la función factor a subset del dataframe de variables discretas.


```{r echo=TRUE, message=FALSE, warning=FALSE}
#Para factorizar selecciono las columnas que creo se deben factorizar
col_disc = colnames(select(pacientes,contains(c("sex","cp","fbs","restecg","exng","slp",
                                                "caa","thall","output"))))
#Aplico la funcion factor y reviso la clase después
pacientes[col_disc] <- lapply(pacientes[col_disc], factor) 
sapply(pacientes, class)
```

## Selección de los datos de interés

La mayoría de los atributos parecen necesarios para analizar su impacto en la probabilidad de sufrir un infarto. Excepto quizás el número de vasos principales, que podríamos eliminar del conjunto de datos.

```{r  message= FALSE, warning=FALSE}
pacientes <- pacientes[,-12]
```

Antes de continuar procedemos a traducir los nombres de las columnas para simplificar la comprensión de la práctica, y volvemos a ver el resumen estadístico con la traducción y las variables factorizadas.

```{r  message= FALSE, warning=FALSE}
names(pacientes) <- c("edad","sexo","dolor_pecho","presion_arterial","colesterol",
                      "azucar","electro","frecuencia_cardiaca","angina_ejercicio",
                      "pico_anterior","pendiente","prueba_esfuerzo",
                      "posibilidad_infarto")
col_disc <- c("sexo",   "dolor_pecho",  "azucar","electro", "angina_ejercicio",
              "pendiente", "prueba_esfuerzo", "posibilidad_infarto")
str(pacientes)
```

Tenemos finalmente 12 atributos y 303 observaciones, las 8 variables factorizadas que son discretas son sexo, dolor_pecho, azucar, electro, angina_ejercicio, pendiente, prueba_esfuerzo y posibilidad de infarto, el resto son continuas.

## Ceros y elementos vacíos

Procedemos a comprobar la existencia de valores ausentes en el dataset.

```{r echo=TRUE, message=FALSE, warning=FALSE}
print('Porcentajes de valores ausentes en cada variable ordenado descendentemente')
sort(colMeans(is.na(pacientes) | pacientes==""), decreasing = TRUE)
cat(ifelse(any(!complete.cases(pacientes)),"SÍ","NO"),"hay valores ausentes")
```

Podemos comprobar como no había valores ausentes. En el caso de haber encontrado que una variable pocos registros con valores ausentes, hubiera bastado con utilizar técnicas de sustitución por un valor por defecto o bien el valor k-vecino más próximo (kNN imputation). En cambio, de haber tenido una variable muchos valores vacíos se podría haber descartado esta sin llegar a perder mucha información.

## Valores extremos

Veamos cuales son los valores outliers para cada variable continua:

```{r  message= FALSE, warning=FALSE}
pacientes_cat <-  pacientes %>% select(col_disc)
col_numericas_cont <- colnames(select(pacientes,!contains(col_disc)))
pacientes_num_cont <- pacientes %>% select(all_of(col_numericas_cont))
pacientes_table <- as.data.frame(apply(pacientes_num_cont, 2, summary))
resultado <- lapply(pacientes_num_cont,function(x) boxplot.stats(x)$out)
resultado
```

También lo podemos visualizar en boxplots. Donde vemos claramente la representación de los outliers anteriores como puntos fuera de los bigotes del boxplot en presion_arterial (6), pico_anterior (4), frecuencia_cariaca (1), colesterol (4).  No hay outliers en edad.

```{r  message= FALSE,out.width="70%", warning=FALSE}
pacientes %>%
  pivot_longer(cols = col_numericas_cont, 
               names_to = "Medida", 
               values_to = "Valor") %>%
  ggplot() +
  geom_boxplot(aes(x = Medida, y = `Valor`, fill = Medida)) +  
  ggtitle("Distribución de variables") +
  coord_flip()
```

Los valores de colesterol y presión arterial son claramente muy altos, por lo que es posible que sean errores. Aun así, ya que estamos viendo la relación de estos valores altos o fuera de la norma y su influencia en la probabilidad de infarto, no los eliminaremos de la muestra original. En consequencia vamos a crear otro dataframe sin estos outliers para eventualmente probar su efecto en los siguientes análisis.

```{r  message= FALSE, warning=FALSE}
#Upper level colesterol
upc_manual <- 400
#Upper level presion arterial
upp_manual <- 150
#Eliminacion registros con outliers en colesterol y presión
pacientes_sinout <- filter(pacientes, pacientes$colesterol <= upc_manual)
pacientes_sinout <- filter(pacientes, pacientes$presion_arterial <= upp_manual)
```

## Normalización

Podríamos normalizar y pasar los valores a la misma escala para mejorar el rendimiento de un posible modelo o visualizar los boxplot más fácilmente, pero perderíamos interpretación de los datos.  Así que decidimos no normalizar las variables continuas.  Un método hubiera sido por el máximo como se ve a continuación.

```{r echo=TRUE, message=FALSE,   warning=FALSE}
#Para normalizar sólo puedo usar las columnas numéricas que filtro
# Definimos la función de normalización por el máximo
nor <-function(x) { (x -min(x))/(max(x)-min(x))}
# Guardamos un nuevo dataset normalizado de variables numéricas para usar en kmeans
pacientes_num_cont_nor <- as.data.frame(lapply(pacientes_num_cont, nor))
```

Veamos como queda la distribución de las variables numéricas del dataset normalizado.  Cuando se usan los boxplot en conjunto para identificar outliers la visualización es mejor si están todas las variables normalizadas.

```{r echo=TRUE, message=FALSE, out.width="70%", warning=FALSE}
pacientes_num_cont_nor %>%
  pivot_longer(cols = col_numericas_cont, 
               names_to = "Medida", 
               values_to = "NormalizadoDiff") %>%
  ggplot() +
  geom_boxplot(aes(x = Medida, y = `NormalizadoDiff`, fill = Medida)) +  
  ggtitle("Distribución de variables normalizadas") +
  coord_flip()
```

## Discretización de variable edad

Categorizamos la variable edad en rangos para poder hacer el estudio estadístico por grupos con ella y vemos su distribución usando la función table.

```{r echo=TRUE, message=FALSE, warning=FALSE}
pacientes["rango_edad"] <- cut(pacientes$edad, breaks = c(0,45,55,65,100), 
                               labels = c("Grupo1:<=45", "Grupo2:46-55", 
                                         "Grupo3:56-65","Grupo3:>65"))
table(pacientes$rango_edad)

```

Los hemos categorizado en los siguientes grupos: Grupo1:<=45, Grupo2:46-55, Grupo3:56-65 y Grupo3:>65.

## Exportación de los datos preprocesados

Una vez que hemos acometido sobre el conjunto de datos inicial los procedimientos de integración, validación y limpieza anteriores, procedemos a guardar estos en un nuevo fichero denominado heart_clean.csv.csv:

```{r  message= FALSE, warning=FALSE}
write.csv(pacientes, "../dataset/heart_clean.csv")
```

# Análisis de los datos

## Selección de los grupos de datos a analizar

Vamos a seleccionar los grupos dentro de nuestro conjunto de datos que pueden resultar interesantes para analizar y/o comparar.  Para esto, utilizaremos la nueva variable discretizada: rango de edad y también usaremos el sexo.

```{r  message= FALSE, warning=FALSE}
# Agrupación por rango de edad: "Grupo1:<=45", "Grupo2:46-55", "Grupo3:56-65",
#"Grupo3:>65"
pacientes.grupoh45 <- pacientes[pacientes$rango_edad == "Grupo1:<=45",]
pacientes.grupoh55 <- pacientes[pacientes$rango_edad == "Grupo2:46-55",]
pacientes.grupoh65 <- pacientes[pacientes$rango_edad == "Grupo3:56-65",]
pacientes.grupom65 <- pacientes[pacientes$rango_edad == "Grupo3:>65",]
# Agrupación por sexo
pacientes.hombres <- pacientes[pacientes$sexo == 1,]
pacientes.mujeres <- pacientes[pacientes$sexo == 0,]
```

## Análisis estadístico descriptivo

Tal y como hemos tenido que hacer con anterioridad para realizar el análisis de ouliters en la fase de limpieza de datos, vemos a continuación los datos estadísticos descriptivos de nuestro dataset de pacientes resultante así como los resúmenes estadísticos de los grupos de dataset que hemos creado para poder compararlos.

```{r  message= FALSE, warning=FALSE}
summary(pacientes)
```

La función summary nos ha dado todos los datos estadísticos necesarios como mínimo, máximo, media, mediana, cuartiles para las variables numéricas continuas y la distribución de las categóricas.  Las visualizaciónes ya realizadas en la primera fase con boxplot nos daban la misma información de manera visual.

Como ya comentamos hay outliers, ausencia de distribución normal y también una distribución desequilibrada en las variables categóricas.

Un análisis visual bivariable usando los rangos de edad y sexo como el que mostramos aquí nos da también de forma más fácil de interpretar el análisis estadístico descriptivo de algunas variables por parejas.

```{r  message= FALSE, out.width="70%", warning=FALSE}
#hacemos de forma automática las tablas para todas las variables categóricas
tbcat <- sapply(pacientes_cat, inherits, "numeric")
tbcat
#creamos lista con todas las tablas de contingencia para cada variable con 
#la objetivo
contg <- lapply(pacientes_cat[!tbcat][-8], table, pacientes_cat[!tbcat][[8]])
#parseamos cada elemento de la lista (una tabla con una variable y la variable 
#objetivo) para poder hacer el plot
Distribucion <- function(y) {
    contg[y]
    col <- names(contg[y])
    list_to_df <- as.data.frame(contg[y])
    names(list_to_df) <- c(col,"Posibilidad_infarto","Freq")
    col2 <- "Posibilidad_infarto"
    col3 <- "Freq"
    gp <- ggplot(list_to_df, aes_string(fill=col2, y=col3, x=col)) +
      geom_bar(position='dodge', stat='identity')  +
      labs(title = paste("Distribución",col), x= col, y= "Número observaciones")
  return(gp)
}
figure <- ggarrange(Distribucion(1), Distribucion(2), Distribucion(3), 
                    Distribucion(4), Distribucion(5), Distribucion(6), 
                    Distribucion(7),
                    labels = c("A", "B", "C"),
                    ncol = 2, nrow = 2)
figure
```

Como nos han mostrado las gráficas anteriores, en estas podemos comprobar las siguientes conclusiones:

* Las mujeres tienen proporcionalmente mas riesgo de sufrir un infarto que los hombres.
* Los pacientes con dolores en el pecho de tipo angina típica muestran poca probabilidad de sufrir un infarto, y los dolores que no son de angina o las anginas atipicas muestran un riesgo alto de posible infarto.
* El azúcar en sangre en ayunas no nos indica mucha diferencia en el riesgo de infarto.
* Los resultados electrocardiográficos en reposo de tener anomalías en la onda ST-T indican posiblidad de infarto.
* Como las anginas producidas por el ejercicio son poco problables de posible infarto, pero las anginas sin origen en el ejercicio si que indican un posible infarto.
* La pendiente del segmento ST de ejercicio máximo plano indica que es poco problable un infarto, pero cuando es pendiente ascendente indica una alta posibilidad de infarto.
* El resultado de la prueba de esfuerzo con talio ~ (0,3) siendo defecto reversible indica poca posibilidad de infarto, pero cuando es normal indica un alta posibilidad de infarto.

## Análisis estadístico inferencial

En este tipo de análisis pretenderemos modelar o inferir cómo es esa población de pacientes de corazón, asumiendo un grado de error en las estimaciones por el hecho de disponer de una muestra reducida de los datos.

### Comprobación de la normalidad y homogeneidad de la varianza

Vamos a ver la distribución de los valores de las variables continuas.

```{r  message= FALSE, out.width="80%", warning=FALSE}
#creamos visualizaciones de histogramas
pacientes_num_cont %>%
  pivot_longer(cols = col_numericas_cont, 
               names_to = "Medida", 
               values_to = "Value") %>%
ggplot( aes(x=Value, color=Medida, fill=Medida)) +
    geom_histogram(alpha=0.6, binwidth = 20) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    
    facet_wrap(~Medida)
```

Por los histogramas vemos que nuestros valores no tienen una distribución normal.  De las variables continuas, excepto colesteros, las otras cuatro variables presentan la cola hacia la derecha que denota una asimetría positiva.  

Con la función `qqnorm` podemos hacer un Q-Q plot para ver si la variable tiene una distribución normal. Si se aleja de la linea roja, como es el caso, querrá decir que no lo es.

```{r  message= FALSE, out.width="80%", warning=FALSE}
op <- par(mfrow=c(2,3))
normal1 <- qqnorm(pacientes$edad, main = "Normal Q-Q Plot for edad");qqline(
  pacientes$edad, col = 2)
normal2 <- qqnorm(pacientes$colesterol, main = "Normal Q-Q Plot for colesterol");
qqline(pacientes$colesterol, col = 2)
normal3 <- qqnorm(pacientes$frecuencia_cardiaca, main = "Normal Q-Q Plot for 
                  frecuencia");qqline(pacientes$frecuencia_cardiaca, col = 2)
normal4 <- qqnorm(pacientes$presion_arterial, main = "Normal Q-Q Plot for 
                  presion");qqline(pacientes$presion_arterial, col = 2)
normal5 <- qqnorm(pacientes$pico_anterior, main = "Normal Q-Q Plot for pico 
                  anterior");qqline(pacientes$pico_anterior, col = 2)
par(op)
```

También vamos a **comprobar la normalidad** con los test estadísticos de **Shapiro-Wilk** considerado uno de los métodos más potentes para contrastar la normalidad. Asumiremos como **hipótesis nula que la población está distribuida normalmente**, si el p-valor es menor al nivel de significancia $\alpha A$ = 0,05, generalmente , entonces la hipótesis nula es rechazada y se concluye que los datos no cuentan con una distribución normal. 

Representamos en un plot los valores p-value y la línea roja correspondiente al valor límite $\alpha A$ .

```{r  message= FALSE, warning=FALSE, out.width="50%"}
shapiro_list <- lapply(pacientes_num_cont, shapiro.test)
shdf <- ldply (sapply(shapiro_list, '[[', 'p.value'), data.frame)
shdf %>% arrange(desc(X..i..))
names(shdf) <- c("Variable","Valor")
shdf$id <- "ShapiroPValue"
pcpv = ggplot(shdf, aes(x=Variable, y=Valor)) + 
    geom_line(size=1, colour = "coral") + 
    geom_point(size=2.5, colour = "coral") + geom_hline(aes(yintercept = 0.05), 
                                                        colour="red") +
    ylab("") + xlab("") + ggtitle("Shapiro-Wilk p-value") + 
    theme_classic()  + theme(axis.text.x=element_text(angle = -90, hjust = 0)) 
print(pcpv)
```

Veamos si el test de **Anderson-Darling** nos da el mismo resultado, esta vez mostramos una lista de las variables que no cumplen la hipótesis nula y por tanto no tienen distribución normal.

```{r  message= FALSE, warning=FALSE}
alpha = 0.05
col.names = colnames(pacientes)
for (i in 1:ncol(pacientes)) {
  if (i == 1) cat("Variables que no siguen una distribución normal:\n")
  if (is.integer(pacientes[,i]) | is.numeric(pacientes[,i])) {
    p_val = ad.test(pacientes[,i])$p.value
    if (p_val < alpha) {
      cat("\n",col.names[i]," con p-value:", p_val)
      # Format output
      if (i < ncol(pacientes) - 1) cat(", ")
      if (i %% 3 == 0) cat("\n")
    }
  }
  }
```

Ahora vamos a estudiar la homogeneidad de varianzas o **homocedasticidad** mediante la aplicación del test **Fligner-Killeen** ya que como hemos comprobado los datos no cumplen con la condición de normalidad. Sino se hubiera podido usar el test de Levene. En ambas pruebas, la **hipótesis nula asume igualdad de varianzas en los diferentes grupos de datos**, por lo que p-valores inferiores al nivel de significancia indicarán heterocedasticidad.

Vamos a analizar la homocedasticidad con todas las variables en los grupos de posibilidad_infarto diferentes.

```{r  message= FALSE, warning=FALSE}
alpha = 0.05
col.names = colnames(pacientes)
for (i in 1:ncol(pacientes)) {
  if (i == 1) cat("Variables que cumplen heterocedasticidad para 
                  posible infarto:\n")
  if (is.integer(pacientes[,i]) | is.numeric(pacientes[,i])) {
    p_val = fligner.test(pacientes[,i] ~ posibilidad_infarto, 
                         data = pacientes)$p.value
    if (p_val < alpha) {
      cat("\n",col.names[i]," con p-value:", p_val)
      # Format output
      if (i < ncol(pacientes) - 1) cat(", ")
      if (i %% 3 == 0) cat("\n")
    }
  }
  }
```

Para edad, frecuencia cardiaca, pico anterior vemos que no obtenemos un p-valor superior a 0,05, y no podemos aceptar la hipótesis de que las varianzas de ambas muestras son homogéneas.

# Pruebas estadísticas

## Correlación de variables

En el caso de nuestras variables para analizar la correlación entre ellas, al no pasar los test de normalidad ni homocedasticidad, debemos usar la correlación de Spearman, que aparece como alternativa no paramétrica. Además para variables categóricas con escala ordinal el método Spearman también es el indicado. Obtendremos valores entre -1 y 1 siendo los extremos la correlación negativa o positiva perfectas.  Probemos entre presión y frecuencia cardiáca.

```{r  message= FALSE, warning=FALSE}
#correlación método pearson
cor.test(pacientes$presion_arterial,pacientes$frecuencia_cardiaca)
#correlación método spearman
cor.test(pacientes$presion_arterial,pacientes$frecuencia_cardiaca, 
         method="spearman")
```

En ambos casos el p-valor no es significativo y el coeficiente de correlación es negativo y superior a 0.04, siendo más optimista el test pearson, aunque el que debe usarse con estos datos es el de Spearman.

Visualicemos un heatmap a ver si coincide con los datos anteriores.

```{r  message= FALSE, warning=FALSE, fig.height=3.5}
if(!require('reshape2')) install.packages('reshape2'); library('reshape2')
qplot(x=Var1, y=Var2, data=melt(cor(pacientes_num_cont,  
                                    method = "spearman")), fill=value, 
      geom="tile",xlab = "variables 1",ylab = "variables 2") +
   theme(axis.text.x = element_text(angle = 90)) +
   coord_fixed()
```

En el gráfico (azul claro) se aprecia una correlación leve entre presión arterial y edad, o colesterol y edad o pico_anterior y edad. Veamos los tests:

```{r  message= FALSE, warning=FALSE}
#correlación método spearman
cor.test(pacientes$presion_arterial,pacientes$edad, method="spearman")
cor.test(pacientes$colesterol,pacientes$edad, method="spearman")
cor.test(pacientes$pico_anterior,pacientes$edad, method="spearman")
```

Efectivamente con edad están más correlacionadas aunque muy levemente con coeficiente de correlación 0.26 versus 0.04 anterior.

## Contraste de hipótesis

### Comparación entre dos grupos de datos

En nuestro dataset de pacientes, puesto que no se cumple la normalidad y homocedasticidad en las variables continuas y como hemos comprobado con los tests paramétricos, para ver que las distribuciones de los grupos de datos que hemos seleccionado anteriormente son las mismas, se deberán aplicar pruebas no paramétricas como **Wilcoxon** (cuando se comparen datos dependientes) o **Mann-Whitney** (cuando los grupos de datos sean independientes).

Vamos a utilizar la función wilcox.test() para realizar las pruebas de Wilcoxon y Mann-Whitney y comparar las distribuciones de todas las variables numéricas continuas y la variable dependiente posibilidad_infarto.

```{r  message= FALSE, out.width="70%", warning=FALSE}
alpha = 0.05
col.names = colnames(pacientes)
for (i in 1:ncol(pacientes)) {
  if (i == 1) cat("Variables con las que se ven diferencias estadísticamente 
                  significativas para posible infarto:\n")
  if (is.integer(pacientes[,i]) | is.numeric(pacientes[,i])) {
    p_val = wilcox.test(pacientes[,i] ~ posibilidad_infarto, 
                        data = pacientes)$p.value
    if (p_val < alpha) {
      cat("\n",col.names[i]," con p-value:", p_val)
      # Format output
      if (i < ncol(pacientes) - 1) cat(", ")
      if (i %% 3 == 0) cat("\n")
    }
  }
  }
```

En este caso, sí se observan diferencias estadísticamente significativas en la todas las variables numéricas entre posibilidad mayor o menor de infarto.

Puesto que tenemos tantas variables categóricas, vamos a ver si existen diferencias significativas en cada variable categórica y los grupos definidos por la variable categórica posibilidad_infarto.  Para ello hay que aplicar el test de **chi^2** en R, mediante la función chisq.test().

```{r  message= FALSE, warning=FALSE, out.width="50%"}
chiq_list <- lapply(contg, chisq.test)
chdf <- ldply (sapply(chiq_list, '[[', 'p.value'), data.frame)
chdf %>% arrange(desc(X..i..))
names(chdf) <- c("Variable","Valor")
chdf$id <- "ChiTestPValue"
pcpv2 = ggplot(chdf, aes(x=Variable, y=Valor)) + 
    geom_line(size=1, colour = "coral") + 
    geom_point(size=2.5, colour = "coral") + geom_hline(aes(yintercept = 0.05), 
                                                        colour="red") +
    ylab("") + xlab("") + ggtitle("Chi-test p-value") + 
    theme_classic()  + theme(axis.text.x=element_text(angle = -90, hjust = 0)) 
print(pcpv2)
```

Como podemos comprobar, en este caso sólo rechazamos **la hipótesis 0, es decir que no hay diferencias estadísticas significativas para los diferentes grupos**, para el azúcar.

## Regresión

Tras haber visto la correlación entre algunas variables numéricas, vamos a analizar con un modelo de regresión lineal la relación entre alguna de ellas: presión arterial y edad, y entre edad y frecuencia cardiáca, así como entre frecuencia cardiaca y presión arterial.

```{r  message= FALSE, warning=FALSE}
m1 <- lm(presion_arterial~edad,data=pacientes)
m2 <- lm(frecuencia_cardiaca~edad,data=pacientes)
m3 <- lm(presion_arterial~frecuencia_cardiaca,data=pacientes)
```

Siendo el coeficiente de determinación (R-squared) una medida de calidad del modelo que toma valores entre 0 y 1, se comprueba cómo se correlacionan muy débilmente las variables seleccionadas, y en el caso de presion_arterial y frecuencia_cardiaca la correlación es negativa.

Podemos usar la regresión logística para analizar la regresión usando la variable dicotómica posibilidad_infarto en función de las demás variables predicatoras.

```{r  message= FALSE, warning=FALSE}
m4<- glm(posibilidad_infarto ~ colesterol+presion_arterial,data=pacientes, 
         family="binomial")
```

Este modelo nos proporciona un AIC (criterio de información de Akaike) de 415.8.  Probemos introduciendo la edad.

```{r  message= FALSE, warning=FALSE}
m5 <- glm(posibilidad_infarto ~ colesterol+presion_arterial+edad,data=pacientes, 
         family="binomial")
summary(m5)
```

Este modelo nos proporciona un AIC (criterio de información de Akaike) de 407.25. Por lo que el modelo anterior (m4) es mejor.

Vamos a usarlo para intentar predecir la variable posibilidad_infarto en nuevos datos.

```{r  message= FALSE, warning=FALSE}
nuevo_paciente <- data.frame(
colesterol = 220,
presion_arterial = 100
)
# Predecir el precio
predict(m4, nuevo_paciente)
```

Parece que el nuevo paciente está más cerca de tener más posibilidades de infarto que de tener menos con un resultado de 0.76.

\newpage
# Conclusiones

En esta práctica hemos analizado el dataset de [Kaggle dataset sobre ataques de corazón](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset), donde nos hemos planteado si alguno de los valores de pruebas realizadas en ellos podría ser determinante en que la posibilidad de infarto sea mayor o menor para el paciente.

Para ello primero hemos ralizado tareas de limpieza de datos, tales como convertir las variables a sus respectivos tipos, filtrar que variables nos interesa utilizar e incluso limpiar el dataset de valores nulos. En el caso de los valores extremos hemos decidido diverger el dataset en uno donde los incluye, para asi poder estudiar todos los casos, y otro donde los excluye, para poder trabajar con datos mas generalizados. Ademas, hemos comprobado como quedarían los valores numéricos si se normalizaban y hemos discretizado la variable edad, para poder trabajar con ella de forma más eficiente.

A continuación hemos realizado un análisis estadístico descriptivo, donde las conclusiones han sido las siguientes: Hay mayor posibilidad de infarto en mujeres y en pacientes con dolores que no son de angina o las anginas atípica con respecto a los otros. El azúcar no tiene importancia significativa en el riesgo de infarto.  Otros indicios de posibilidad de infarto alta son: tener anomalías en la onda ST-T en el electrocardiograma, las anginas de origen distinto al ejercicio,  una pendiente ascendiente y una prueba de esfuerzo con talio ~ (0,3) normal.

Tambien hemos realizado un análisis estadístico inferencial, y hemos podido comprobar que ninguna de las variables numéricas ha pasado el test de distribución normal ni homocedasticidad, por ello los test analíticos realizados han sido no paramétricos. 

Para finalizar, hemos realizado tres tipos de pruebas estadísticas al conjunto de datos para poder conocer nuestras muestras, saber cuanta información proveen con respecto a la población.  

Las pruebas han sido buscar correlación de variables, el contraste de distintas hipótesis (diferencias estadísticamente significativas) y generar un modelo de regresión para predecir si un paciente tiene riesgo a tener un infarto. Las conclusiones que hemos sacado en base a los resultados obtenidos han sido las siguientes:

* El análisis de correlación nos ha permitido averiguar que no hay correlación fuerte entre las variables ni con la variable objetivo.  
* El contraste de hipótesis nos ha permitido conocer que, menos la variable de nivel de azucar en sangre, con todas las variables rechazaríamos la hipótesis nula. Dicho de otro modo, estas variables ejercen una mayor influencia sobre la posibilidad de infarto.  
* El modelo de regresión logística obtenido parece no tener calidad aunque hemos podido utilizarlo para predecir la posibilidad de infarto en un nuevo paciente.

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Contribuciones        | Firma    |
|---------------|:-------------:|------:|
| Investigación previa      | Enrique, Inma|
| Redacción de las respuestas      | Enrique, Inma |
| Desarrollo del código | Enrique, Inma |
| Participación en el vídeo | Enrique, Inma |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

# Bibliografía

* Descripción de variables dataset: <https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/discussion/329925>
* Test for homogeneity of variances: <https://biostats.w.uib.no/test-for-homogeneity-of-variances-levenes-test/>
* Data science concepts you need to know! Part 1: <https://towardsdatascience.com/introduction-to-statistics-e9d72d818745>
* Introducción a la limpieza y análisis de los datos: Calvo M, Subirats L, Pérez D (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.